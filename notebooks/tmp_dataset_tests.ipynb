{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspace/notebooks\n",
      "Working directory changed to: /workspace\n",
      "[2025-04-19 19:21:50,460] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "os.chdir(\"/workspace\")\n",
    "print(f\"Working directory changed to: {os.getcwd()}\")\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import verifiers as vf\n",
    "from verifiers.envs.bfcl_inthinking_env import preprocess_bfcl_dataset, BFCL_INTHINKING_USER_PROMPT\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c10ba763bf4baa9587560f5ffade78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fe5f0c625d4f6d83d711fed5eae63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b633fd4c215c4f919a54b39b20f2b71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'initial_config', 'path', 'involved_classes', 'num_turns', 'answer', 'prompt', 'user_question_bank', 'ground_truth_bank'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testds=preprocess_bfcl_dataset(curriculum_learning=False, split=\"train\")\n",
    "testds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import importlib\n",
    "import inspect\n",
    "import json\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch # Added for tokenization\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    DatasetDict,  # type: ignore\n",
    ")\n",
    "from huanzhi_utils import load_file\n",
    "from loguru import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trl.trainer.grpo_trainer import RewardFunc\n",
    "from transformers import PreTrainedTokenizerBase # Added for tokenization\n",
    "\n",
    "from verifiers.envs.multistep_env import MultiStepEnv\n",
    "from verifiers.envs.tool_env import infer_schema_from_function\n",
    "from verifiers.parsers import XMLParser\n",
    "from verifiers.rubrics.bfcl_inthinking_rubric import BfclITRubric\n",
    "from verifiers.tools.bfcl_tools import (\n",
    "    INVOLVED_CLASS_TO_FUNC_DOC_PATH,\n",
    "    construct_tools_from_involved_classes,\n",
    ")\n",
    "\n",
    "from verifiers.imports import LLM, SamplingParams  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_bfcl_prompt(\n",
    "    involved_classes: List[str] | None = None,\n",
    "    user_question: str | None = None,\n",
    ") -> List[Dict[str, str]]:\n",
    "    messages = []\n",
    "    tools = construct_tools_from_involved_classes(involved_classes)\n",
    "    # Combine instructions, tools, and query into the first user message\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": BFCL_INTHINKING_USER_PROMPT.format(\n",
    "                tools=tools, user_query=user_question\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "curriculum_learning=False\n",
    "multi_turn_base_data = load_file(\n",
    "    \"verifiers/berkeley-function-call-leaderboard/data/BFCL_v3_multi_turn_base.json\"\n",
    ")\n",
    "multi_turn_base_answer = load_file(\n",
    "    \"verifiers/berkeley-function-call-leaderboard/data/possible_answer/BFCL_v3_multi_turn_base.json\"\n",
    ")\n",
    "\n",
    "# Reprocess the columns into serializable format and add num_turns\n",
    "processed_data = []\n",
    "for i in range(len(multi_turn_base_data)):\n",
    "    entry_data = multi_turn_base_data[i]\n",
    "    entry_answer = multi_turn_base_answer[i]\n",
    "\n",
    "    # --- Handle Multi-Turn Tasks ---\n",
    "    # If a task has multiple user turns, create separate data points for each turn.\n",
    "    # Each data point will contain only the *first* user question for that turn's context.\n",
    "    # The ground truth will correspond to the actions needed for *that specific turn*.\n",
    "    questions = entry_data[\"question\"]\n",
    "    ground_truths = entry_answer[\"ground_truth\"]\n",
    "    initial_config = entry_data[\"initial_config\"]\n",
    "    involved_classes = entry_data[\"involved_classes\"]\n",
    "    entry_id = entry_data[\"id\"]\n",
    "\n",
    "    assert len(questions) == len(ground_truths), (\n",
    "        f\"Mismatch in number of turns for entry {entry_id}\"\n",
    "    )\n",
    "\n",
    "    # Create one entry per original turn\n",
    "    for turn_idx in range(len(questions)):\n",
    "        # The prompt only contains the *first* user question of the *current* turn\n",
    "        current_user_question = questions[turn_idx][0][\"content\"]\n",
    "        # The ground truth is only for the *current* turn\n",
    "        current_ground_truth = [ground_truths[turn_idx]] # Wrap in list for consistency\n",
    "\n",
    "        processed_entry = {\n",
    "            \"id\": f\"{entry_id}_turn_{turn_idx}\", # Unique ID per turn\n",
    "            \"involved_classes\": involved_classes,\n",
    "            \"initial_config\": json.dumps(initial_config), # Keep original initial config\n",
    "            \"prompt\": format_bfcl_prompt(\n",
    "                involved_classes=involved_classes,\n",
    "                user_question=current_user_question,\n",
    "            ),\n",
    "            # Ground truth now only contains the answer for the *current* turn's request\n",
    "            \"answer\": json.dumps(current_ground_truth),\n",
    "            # Store the original full answer for potential complex reward calculation later if needed\n",
    "            \"original_full_answer\": json.dumps(ground_truths),\n",
    "            \"num_total_turns_in_task\": len(questions), # Original number of turns in the task\n",
    "            \"current_turn_index\": turn_idx, # Index of this turn within the original task\n",
    "            # These are no longer needed as we process one turn at a time\n",
    "            # \"user_question_bank\": \"[]\",\n",
    "            # \"ground_truth_bank\": \"[]\",\n",
    "        }\n",
    "        processed_data.append(processed_entry)\n",
    "\n",
    "        # If curriculum learning, potentially create sub-entries (though less relevant with single-turn interaction)\n",
    "        # For now, curriculum learning based on num_total_turns_in_task\n",
    "        if curriculum_learning:\n",
    "                processed_entry[\"num_turns\"] = processed_entry[\"num_total_turns_in_task\"] # Use total task turns for curriculum sorting\n",
    "\n",
    "\n",
    "if not curriculum_learning:\n",
    "    # Add num_turns if not using curriculum learning (can just be 1 or based on original task)\n",
    "        for entry in processed_data:\n",
    "            entry[\"num_turns\"] = entry[\"num_total_turns_in_task\"] # Or set to 1 if preferred\n",
    "\n",
    "\n",
    "dataset = Dataset.from_list(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'involved_classes', 'initial_config', 'prompt', 'answer', 'original_full_answer', 'num_total_turns_in_task', 'current_turn_index', 'num_turns'],\n",
       "    num_rows: 745\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987036a6c77e4cf9b17aae2786fc4853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4dbd73faaa4c66a7428145dcdd6eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get unique IDs and split those first\n",
    "unique_ids = sorted(list(set(dataset[\"id\"])))\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "# Filter dataset based on IDs\n",
    "train_dataset = dataset.filter(lambda x: x[\"id\"] in train_ids)\n",
    "test_dataset = dataset.filter(lambda x: x[\"id\"] in test_ids)\n",
    "\n",
    "if curriculum_learning:\n",
    "    # Sort both splits by num_turns while preserving randomization within same num_turns\n",
    "    def sort_by_turns(split):\n",
    "        df = split.to_pandas()\n",
    "        # Set seed for reproducibility\n",
    "        rng = np.random.RandomState(42)\n",
    "        # Randomize order within same num_turns by adding small random values\n",
    "        df[\"sort_key\"] = df[\"num_turns\"] + rng.random(len(df)) * 0.1\n",
    "        df = df.sort_values(\"sort_key\")\n",
    "        df = df.drop(\"sort_key\", axis=1)\n",
    "        return Dataset.from_pandas(df)\n",
    "\n",
    "    train_dataset = sort_by_turns(train_dataset)\n",
    "    test_dataset = sort_by_turns(test_dataset)\n",
    "\n",
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "\n",
    "# assert train_dataset and test_dataset have non-overlapping ids\n",
    "assert len(set(train_dataset[\"id\"]) & set(test_dataset[\"id\"])) == 0, (\n",
    "    \"Train and test datasets have overlapping ids\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'involved_classes', 'initial_config', 'prompt', 'answer', 'original_full_answer', 'num_total_turns_in_task', 'current_turn_index', 'num_turns'],\n",
       "    num_rows: 372\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_max_turns = 1\n",
    "if effective_max_turns > 0:\n",
    "        logger.info(f\"Filtering train dataset to max {effective_max_turns} total turns in task.\")\n",
    "        self.dataset = self.dataset.filter(\n",
    "            lambda x: x[\"num_total_turns_in_task\"] <= effective_max_turns\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1945b4cca1a42eda7af2ee2c120632f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'involved_classes', 'initial_config', 'prompt', 'answer', 'original_full_answer', 'num_total_turns_in_task', 'current_turn_index', 'num_turns'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.filter(\n",
    "            lambda x: x[\"num_total_turns_in_task\"] <= 1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'involved_classes', 'initial_config', 'prompt', 'answer', 'original_full_answer', 'num_total_turns_in_task', 'current_turn_index', 'num_turns'],\n",
       "    num_rows: 373\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verifiers.utils.data_utils import preprocess_bfcl_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5362e20ccf814ea1b1f75ef324068c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a1f924f0c74c1dbc9734a90ba4d607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b9c3cff4164163a7d1d0398da6e207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test2=preprocess_bfcl_dataset(curriculum_learning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'initial_config', 'path', 'involved_classes', 'num_turns', 'answer', 'prompt', 'user_question_bank', 'ground_truth_bank'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'initial_config', 'path', 'involved_classes', 'num_turns', 'answer', 'prompt', 'user_question_bank', 'ground_truth_bank'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Splitting ---\n",
    "# Get unique original IDs and split those first to keep turns from the same task together\n",
    "unique_original_ids = sorted(list(set(d[\"id\"].split('_turn_')[0] for d in processed_data)))\n",
    "len(unique_original_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_test_split(unique_original_ids, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter dataset based on original IDs\n",
    "train_dataset = dataset.filter(lambda x: x[\"id\"].split('_turn_')[0] in train_ids)\n",
    "test_dataset = dataset.filter(lambda x: x[\"id\"].split('_turn_')[0] in test_ids)\n",
    "\n",
    "if curriculum_learning:\n",
    "    # Sort both splits by num_turns (total task turns)\n",
    "    def sort_by_turns(split_ds):\n",
    "        df = split_ds.to_pandas()\n",
    "        rng = np.random.RandomState(42)\n",
    "        df[\"sort_key\"] = df[\"num_turns\"] + rng.random(len(df)) * 0.1\n",
    "        df = df.sort_values(\"sort_key\")\n",
    "        df = df.drop(\"sort_key\", axis=1)\n",
    "        return Dataset.from_pandas(df)\n",
    "\n",
    "    train_dataset = sort_by_turns(train_dataset)\n",
    "    test_dataset = sort_by_turns(test_dataset)\n",
    "\n",
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "\n",
    "# assert train_dataset and test_dataset have non-overlapping original ids\n",
    "train_original_ids = set(x[\"id\"].split('_turn_')[0] for x in train_dataset)\n",
    "test_original_ids = set(x[\"id\"].split('_turn_')[0] for x in test_dataset)\n",
    "assert len(train_original_ids & test_original_ids) == 0, (\n",
    "    \"Train and test datasets have overlapping original task ids\"\n",
    ")\n",
    "logger.info(f\"Preprocessed dataset for split '{split}'. Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
